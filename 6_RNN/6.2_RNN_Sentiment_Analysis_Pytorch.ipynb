{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN을 이용한 네이버 영화 리뷰 감정 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Download : https://github.com/e9t/nsmc/\n",
    "\n",
    "- ratings_train.txt\n",
    "- ratinns_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. txt Dataset을 csv로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                               text  label\n",
      "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
      "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
      "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
      "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
      "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['id', 'text', 'label']\n",
    "train_data = pd.read_csv('../txt_datasets/ratings_train.txt', sep='\\t', names=columns, skiprows=1).dropna() # null data 삭제\n",
    "test_data = pd.read_csv('../txt_datasets/ratings_test.txt', sep='\\t', names=columns, skiprows=1).dropna()\n",
    "\n",
    "# csv 파일로 변환\n",
    "train_data.to_csv('../csv_datasets/ratings_train.csv', index=False)\n",
    "test_data.to_csv('../csv_datasets/ratings_test.csv', index=False)\n",
    "\n",
    "\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1fc925e85f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Field를 지정한다. 한글 데이터를 다루므로 토크나이저로 spacy를 쓸 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt  # 형태소 분석기중 하나인 Okt를 불러온다\n",
    "Okt = Okt()  # Okt 클래스의 인스턴스 생성. 이 인스턴스를 사용하여 텍스트를 형태소로 분리할 수 있다.\n",
    "\n",
    "# torchtext에 내장된 데이터셋을 이용하는 게 아니므로, 각 컬럼별로 해당하는 Field를 지정해줘야 한다.\n",
    "TEXT = data.Field(tokenize=Okt.morphs) # data.Field는 텍스트 데이터를 어떻게 처리할지 정의한다. torkenize 인자로 Okt.morphs를 사용하고 있으므로, 각 문장을 형태소로 분리한다.\n",
    "LABEL = data.LabelField(dtype = torch.float) # data.LabelField는 label을 어떻게 처리할지 정의한다.\n",
    "\n",
    "# 위에서 정의한 TEXT와 LABEL은 이후 과정에서 데이터를 전처리하는데 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일의 각 컬럼이 어떻게 처리될지를 정의하는 딕셔너리를 생성한다.\n",
    "# 'text' 컬럼은 TEXT Field를 사용하여 처리되고, 'label' 컬럼은 LABEL 필드를 사용하여 처리된다.\n",
    "fields = {'text' : ('text', TEXT), 'label' : ('label', LABEL)}\n",
    "\n",
    "# dictionary 형식은 {csv 컬럼명 : (데이터 컬럼명, Field 이름)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabularDataset 은 데이터를 불러오면서 필드에서 정의했던 토큰화 방법으로 토큰화를 수행한다.\n",
    "train_data, test_data = data.TabularDataset.splits(\n",
    "    path = '../csv_datasets',\n",
    "    train = 'ratings_train.csv',\n",
    "    test = 'ratings_test.csv',\n",
    "    format = 'csv',\n",
    "    fields = fields, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'text': ['아', '더빙', '..', '진짜', '짜증나네요', '목소리'], 'label': '0'},\n",
       " {'text': ['흠',\n",
       "   '...',\n",
       "   '포스터',\n",
       "   '보고',\n",
       "   '초딩',\n",
       "   '영화',\n",
       "   '줄',\n",
       "   '....',\n",
       "   '오버',\n",
       "   '연기',\n",
       "   '조차',\n",
       "   '가볍지',\n",
       "   '않구나'],\n",
       "  'label': '1'},\n",
       " {'text': ['너', '무재', '밓었', '다그', '래서', '보는것을', '추천', '한', '다'], 'label': '0'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불러온 데이터의 형태 확인\n",
    "vars(train_data[0]), vars(train_data[1]), vars(train_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 수 : 149995\n",
      "테스트 데이터 수 : 49997\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터의 갯수 확인\n",
    "print(\"훈련 데이터 수 : {}\".format(len(train_data)))\n",
    "print(\"테스트 데이터 수 : {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터에 검증 데이터가 따로 주어져 있지 않으므로 생성해준다.\n",
    "import random\n",
    "random.seed(SEED)\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))\n",
    "# torchtext의 split 메서드는 기본적으로 데이터를 7:3 비율로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 수 :104996\n",
      "검증 데이터 수 :44999\n",
      "테스트 데이터 수 :49997\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련 데이터 수 :{}\".format(len(train_data)))\n",
    "print(\"검증 데이터 수 :{}\".format(len(valid_data)))\n",
    "print(\"테스트 데이터 수 :{}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85768\n"
     ]
    }
   ],
   "source": [
    "# 총 단어가 몇 개인지 확인 \n",
    "# Field 객체의 build_vocab는 주어진 데이터셋에 대한 단어장(vocabulary)을 만든다. 이는 각 단어를 고유한 정수 인덱스에 매핑한다.\n",
    "TEXT.build_vocab(train_data) # 모든 단어를 단어장에 포함시킨다. \n",
    "\n",
    "print(len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38586"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 중 최소 두 번 이상 등장하는 단어의 갯수 확인\n",
    "TEXT.build_vocab(train_data, min_freq = 2) # 최소 두 번 이상 등장하는 단어만 단어장에 포함시킨다.\n",
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35000개로 단어를 끊는다\n",
    "MAX_VOCAB_SIZE = 35000\n",
    "\n",
    "# 단어장의 크기를 35000개로 제한한다. 만약 단어장에 더 많은 단어가 있으면, 빈도 수가 낮은 단어부터 제외된다.\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE) \n",
    "LABEL.build_vocab(train_data) # 라벨에 대해서도 단어장을 만든다. 이진 분류 문제이므로 라벨 단어장의 크기는 2이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT 단어장의 갯수 : 35002\n",
      "LABEL 단어장의 갯수 : 2\n"
     ]
    }
   ],
   "source": [
    "# TEXT, LABEL 단어장의 갯수 확인\n",
    "\n",
    "print(\"TEXT 단어장의 갯수 : {}\".format(len(TEXT.vocab)))\n",
    "print(\"LABEL 단어장의 갯수 : {}\".format(len(LABEL.vocab)))\n",
    "\n",
    "# <unk>와 <pad> 토큰이 추가되어 있으므로 단어의 갯수가 35,002개이다. \n",
    "# <unk> : Unknown을 나타내며, 단어장에 없는 단어를 대체한다. train data에 등장하지 않은 단어가 test data에서 나타나면 해당 단어는 <unk>로 처리된다\n",
    "# <pad> : Padding을 나타내며, 배치 처리를 위해 모든 문장을 동일한 길이로 만들어야 할 때, 짧은 문장에 패딩을 추가하기 위해 사용된다. \n",
    "# 패딩은 실제 의미를 가지지 않는 토큰으로 문장의 길이를 맞춘다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최빈 단어들이 어떤 것인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 47281), ('이', 39303), ('영화', 35343), ('의', 21608), ('..', 20398), ('가', 19293), ('에', 18781), ('을', 16252), ('...', 16017), ('도', 15031), ('들', 13325), (',', 12338), ('는', 12315), ('를', 11313), ('은', 11161), ('너무', 7753), ('?', 7739), ('한', 7676), ('다', 7146), ('정말', 6899)]\n"
     ]
    }
   ],
   "source": [
    "# 최빈 단어 20개 확인\n",
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어와 인덱스 사이 매핑 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '.', '이', '영화', '의', '..', '가', '에', '을']\n",
      "defaultdict(None, {'0': 0, '1': 1})\n"
     ]
    }
   ],
   "source": [
    "# itos(integer to string) : 정수 인덱스를 단어로 매핑하는 리스트\n",
    "# 여기서 처음 10개의 요소를 출력하면, 단어장의 상위 10개 단어를 볼 수 있다.\n",
    "print(TEXT.vocab.itos[:10]) \n",
    "\n",
    "# 단어를 정수로 매핑\n",
    "# stoi(string to integer) : 단어를 정수 인덱스로 매핑하는 딕셔너리\n",
    "# LABEL의 경우 레이블(클래스)에 대한 매핑을 나타낸다.\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BucketIterator를 이용하여 데이터 생성자를 만든다\n",
    "# 데이터를 배치로 나누고 반복할 수 있는 iterator를 생성하는 과정\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits( \n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65, 64])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 생성된 데이터의 크기 확인\n",
    "\n",
    "next(iter(train_iterator)).text.shape\n",
    "# [문장의 길이 * 배치 사이즈]\n",
    "\n",
    "# BucketIterator는 배치 내의 문장을 가능한 같은 길이로 만들기 위해 패딩을 사용한다.\n",
    "# 따라서 배치 내의 모든 문장은 같은 길이인 65로 만들어진다.\n",
    "# 만약 실제 문장의 길이가 65보다 작다면, 패딩 토큰이 추가되어 문장의 길이가 65가 된다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 바닐라 RNN을 이용하여 모델 생성  \n",
    "\n",
    "모델의 각 input/output 벡터의 사이즈는 다음과 같다\n",
    "- text : [sentence length, batch size]\n",
    "- embedded : [sentence length, batch size, embedding dim]\n",
    "- output : [sentence length, batch size, hidden dim]\n",
    "- hidden : [1, batch size, hidden dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        # input_dim : 단어의 개수(TEXT 사전의 길이)\n",
    "        # embedding_dim : 임베딩은 입력 차원을 연속 (밀집)벡터 공간으로 변환하는 것. 임베딩 차원은 이 벡터 공간의 크기를 의미\n",
    "        # 이렇게 변환된 벡터는 다음 레이어 RNN으로 전달된다.\n",
    "        # hidden_dim : RNN의 은닉 상태의 차원\n",
    "        # output_dim : 최종 출력 차원\n",
    "        \n",
    "        super().__init__()  # 참고 : super(RNN, self).__init__()은 Python2 스타일의 문법. python3에서는 이렇게 해도 됨\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim) # 입력 단어를 연속 (밀집)벡터 공간으로 변환\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim) # 입력 시퀀스의 각 원소에 대해 hidden state를 Update\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) # 최종 출력을 생성\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # 입력 text : [sent_len, batch_size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        # 임베딩 계층을 통과한 후의 출력 = [sent_len, batch_size, emb_dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded) \n",
    "        # output : [sent_len, batch_size, hidden_dim]\n",
    "        # hidden : [1, batch_size, hidden_dim]\n",
    "        \n",
    "        assert torch.equal(output[-1, :, :], hidden.squeeze(0))  \n",
    "        # output의 마지막 슬라이스와 hidden이 동일한지 확인한다.\n",
    "        # hidden 텐서는 시퀀스의 마지막 시간 단계에서의 hidden state를 담고 있음. hidden.squeeze(0) 는 [batch_size, hidden_dim] 형태\n",
    "        # output 텐서는 각 시간 단계의 hidden state를 저장함. 따라서 output[-1, :, :]는 시퀀스의 마지막 시간 단계에서의 hidden state를 나타냄\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0)) # [batch_size, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 모델은 3592105개의 파라미터를 가지고 있다\n"
     ]
    }
   ],
   "source": [
    "# 모델의 파라미터 수 추출\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"이 모델은 {}개의 파라미터를 가지고 있다\".format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function : binary cross entropy with logits\n",
    "# BCEWithLogitLoss() : 임의의 실수를 입력으로 받아서 sigmoid 함수를 취해, 0과 1 사이의 값으로 변환한 뒤 label과 BCE를 계산한다\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 손실함수를 GPU에 올린다\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가를 위해 임의의 실수를 0과 1 두 정수 중 하나로 변환하는 함수 만들기\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 훈련과 평가를 위한 함수 만들기\n",
    "# model의 output size는 [batch size, output_dim] 인데, output_dim = 1이므로 이 차원을 없애줘야 label과 비교할 수 있다. 따라서 squeeze(1) 적용한다\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가를 위한 함수는 그래디언트 업데이트를 하지 않아야 하므로 with torch.no_grad(): 구문으로 감싼다\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch마다 걸린 훈련 시간을 측정하는 함수 만들기\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/5 | time : 0m 11s\n",
      "train loss : 0.6929226821330464, train acc : 50.26025797157009\n",
      "Val_loss : 0.6883356756615367, Val_acc : 54.45350548252463\n",
      "epoch : 2/5 | time : 0m 11s\n",
      "train loss : 0.6929856560202651, train acc : 50.30342271094058\n",
      "Val_loss : 0.6883635412562977, Val_acc : 54.59491680376232\n",
      "epoch : 3/5 | time : 0m 10s\n",
      "train loss : 0.6930777966649266, train acc : 49.93863836575542\n",
      "Val_loss : 0.6882084793495861, Val_acc : 54.61077009412375\n",
      "epoch : 4/5 | time : 0m 10s\n",
      "train loss : 0.6929940632535945, train acc : 50.299190873783004\n",
      "Val_loss : 0.6881241702728651, Val_acc : 54.635501221161\n",
      "epoch : 5/5 | time : 0m 10s\n",
      "train loss : 0.6929318275777107, train acc : 50.02285191956124\n",
      "Val_loss : 0.6881870889020237, Val_acc : 54.69098772684281\n"
     ]
    }
   ],
   "source": [
    "# 실제 모델 훈련, 검증 셋의 손실이 개선되면 모델을 저장하도록 구현\n",
    "\n",
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf') # 초기에는 무한대로 설정됨\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss :\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'checkpoint/2.1_RNN_Sentiment_Analysis.pt')\n",
    "        \n",
    "    print(\"epoch : {}/{} | time : {}m {}s\".format(\n",
    "        epoch+1, N_EPOCHS, epoch_mins, epoch_secs\n",
    "    ))\n",
    "    print(\"train loss : {}, train acc : {}\".format(train_loss, train_acc * 100))\n",
    "    print(\"Val_loss : {}, Val_acc : {}\".format(valid_loss, valid_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능이 좋지 않다 -> 6.3 : Multi-layer bi-directional LSTM 을 이용해서 진행"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1_8",
   "language": "python",
   "name": "torch1_8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
